ggraph::ggraph(corMat)
ggraph::ggraph(corMat %>% as.data.frame)
corMat %>% as.data.frame
corMat %>% data.frame
corMat %>% data.frame %>% str
library('igraph')
corMat <- dfm_trim(dfMat, min_termfreq = 10) %>% as.matrix %>% cor
corMat[abs(corMat) < 0.1] <- 0
corGraph <- graph_from_adjacency_matrix(corMat, mode = 'directed', weighted = NULL, diag = F)
plot(corMat)
corGraph
corGraph <- graph_from_adjacency_matrix(corMat)#, mode = 'directed', weighted = NULL, diag = F)
plot(corGraph)
plot(corGraph, layout = 'fr')
plot(corMat, layout = 'fr')
corMat <- dfm_trim(dfMat, min_termfreq = 10) %>% as.matrix %>% cor
corMat[abs(corMat) < 0.9] <- 0
corGraph <- graph_from_adjacency_matrix(corMat)#, mode = 'directed', weighted = NULL, diag = F)
plot(corGraph)
corMat <- dfm_trim(dfMat, min_termfreq = 100) %>% as.matrix %>% cor
corMat[abs(corMat) < 0.9] <- 0
corGraph <- graph_from_adjacency_matrix(corMat)#, mode = 'directed', weighted = NULL, diag = F)
plot(corGraph)
corMat <- dfm_trim(dfMat, min_termfreq = 100) %>% as.matrix %>% cor
corMat[abs(corMat) < 0.9] <- 0
corGraph <- graph_from_adjacency_matrix(corMat, mode = 'directed', weighted = NULL, diag = F)
plot(corGraph)
corGraph <- graph_from_adjacency_matrix(corMat, mode = 'undirected', weighted = T, diag = F)
plot(corGraph)
corGraph <- graph_from_adjacency_matrix(corMat, mode = 'directed', weighted = T, diag = F)
plot(corGraph)
plot(corGraph, layout = layout_in_circle)
ggraph(corGraph, layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point() +
geom_node_text(aes(label = name, size = 8), repel = TRUE) +
theme_void()
ggraph::ggraph(corGraph, layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point() +
geom_node_text(aes(label = name, size = 8), repel = TRUE) +
theme_void()
library('ggraph')
ggraph(corGraph, layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point() +
geom_node_text(aes(label = name, size = 8), repel = TRUE) +
theme_void()
ggraph(corGraph, layout = "fr") +
geom_edge_link(show.legend = FALSE) +
geom_node_point() +
geom_node_text(aes(label = name, size = 8), repel = TRUE) +
theme_void()
graph_from_data_frame(m) %>%
induced_subgraph(degree(.) > 2) %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point() +
geom_node_text(aes(label = name, size = 8), repel = TRUE) +
theme_void()
m <- corMat
m <- data.frame(row=rownames(m)[row(m)], col=colnames(m)[col(m)], corr=c(m))
graph_from_data_frame(m) %>%
induced_subgraph(degree(.) > 2) %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point() +
geom_node_text(aes(label = name, size = 8), repel = TRUE) +
theme_void()
graph_from_data_frame(m) %>%
induced_subgraph(degree(.) > 2) %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = corr), show.legend = FALSE) +
geom_node_point() +
geom_node_text(aes(label = name, size = 8), repel = TRUE) +
theme_void()
Docs <- sapply(list.files(pattern = '.pdf*'), pdf_text)
## Remove some pages
Pages <- list(6:79,
c(17:91, 98:122, 126:155, 159:169),
c(16:86, 93:120, 125:162, 167:199))
for (i in seq_along(Docs)) Docs[[i]] <- Docs[[i]][Pages[[i]]]
## Collapse pages
Docs %<>% sapply(paste, collapse = '')
## Split by activity
splitPat <- '\n\\s*\\p{Uppercase}{2,4}.*-\\d{1,2}-\\d{4,}.*(.*\n){1,5}\\s*Specific' # Define pattern to split by
str_extract_all(Docs[[3]], splitPat) # Examine
Acts <- str_split(Docs, splitPat) %>% lapply(`[`, -1) # Remove part before 1st activity
## Tidy
Acts <- unlist(Acts)
### Add period as an attribute
attributes(Acts) <- str_extract_all(Docs, splitPat) %>% sapply(length) %>%
rep(c("1415", "1617", "1820"), times = .) %>% list(period = .)
### Add document names
names(Acts) <- str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n|\n\\s*', '', .) %>% # Remove newline characters
gsub('\\s{2,}', '\\s', .)
names(Acts)
names(Acts) <- str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n|\n\\s*', '', .)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n|\n\\s*', '', .)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n\\s*', '', .)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', '', .)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n\\s+', '', .)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\\s+\n\\s+', '', .)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\\s*\n\\s*', '', .)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', '', .)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', '', .) %>% # Remove newline characters
gsub('\\s{2,}', '\\s', .) %>% # Replace multiple spaces with one
sub('\\d*Specific$', '', .)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', '', .) %>% # Remove newline characters
gsub('\\s{2,}', ' ', .) %>% # Replace multiple spaces with one
sub('\\d*Specific$', '', .)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', '', .) %>% # Remove newline characters
gsub('\\s{2,}', ' ', .) %>% # Replace multiple spaces with one
gsub('^\\s*|\\s*$', '', .) %>% # Remove
sub('\\d*Specific$', '', .)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', '', .) %>% # Remove newline characters
gsub('\\s+', ' ', .) %>% # Replace spaces with one
gsub('^\\s*|\\s*$', '', .) %>% # Remove leading and trailing spaces
sub('\\d*Specific$', '', .)
str_extract_all(Docs, splitPat) %>% unlist
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', ' ', .) %>% # Replcae newline characters with space
gsub('\\s+', ' ', .) %>% # Replace spaces with one
gsub('^\\s*|\\s*$', '', .) %>% # Remove leading and trailing spaces
sub('\\d*Specific$', '', .) # Remove footnote numbers and "Specific"
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', ' ', .) %>% # Replcae newline characters with space
gsub('\\s+', ' ', .) %>% # Replace spaces with one
sub('\\d*Specific$', '', .) %>% # Remove footnote numbers and "Specific"
gsub('^\\s*|\\s*$', '', .)# Remove leading and trailing spaces
#? Created on 2020-09-28 15:16:20 by Jyri Lillemets with R version 4.0.2 (2020-06-22).
#? This script ...
# Set options
#options(device = 'X11')
# Set working directory
setwd('/home/jrl/work/bioeastsup/horizon')
#setwd('C:/Users/lillemets/Google Drive/')
# Load packages
library('magrittr')
library('pdftools');library('stringr')
library('quanteda')
# Download and tidy documents (2020-09-28 15:20:10) ----------
## Load
if (length(list.files(pattern = '.pdf*')) == 0) {
Urls <- c('https://ec.europa.eu/research/participants/data/ref/h2020/wp/2014_2015/main/h2020-wp1415-food_en.pdf',
'https://ec.europa.eu/research/participants/data/ref/h2020/wp/2016_2017/main/h2020-wp1617-food_en.pdf',
'https://ec.europa.eu/research/participants/data/ref/h2020/wp/2018-2020/main/h2020-wp1820-food_en.pdf')
lapply(Urls, function(x) download.file(x, sub('.*\\/','', x)))
}
Docs <- sapply(list.files(pattern = '.pdf*'), pdf_text)
## Remove some pages
Pages <- list(6:79,
c(17:91, 98:122, 126:155, 159:169),
c(16:86, 93:120, 125:162, 167:199))
for (i in seq_along(Docs)) Docs[[i]] <- Docs[[i]][Pages[[i]]]
## Collapse pages
Docs %<>% sapply(paste, collapse = '')
## Split by activity
splitPat <- '\n\\s*\\p{Uppercase}{2,4}.*-\\d{1,2}-\\d{4,}.*(.*\n){1,5}\\s*Specific' # Define pattern to split by
str_extract_all(Docs[[3]], splitPat) # Examine
Acts <- str_split(Docs, splitPat) %>% lapply(`[`, -1) # Remove part before 1st activity
## Tidy
Acts <- unlist(Acts)
### Add period as an attribute
attributes(Acts) <- str_extract_all(Docs, splitPat) %>% sapply(length) %>%
rep(c("1415", "1617", "1820"), times = .) %>% list(period = .)
### Add document names
names(Acts) <- str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', ' ', .) %>% # Replcae newline characters with space
gsub('\\s+', ' ', .) %>% # Replace spaces with one
sub('\\d*Specific$', '', .) %>% # Remove footnote numbers and "Specific"
gsub('^\\s*|\\s*$', '', .)# Remove leading and trailing spaces
## Save
saveRDS(Acts, 'texts.Rds')
# Make corpus, tokens, DFM and FCM (2020-09-28 17:38:58) ----------
## Read data
if (!exists('Acts')) Acts <- readRDS('texts.Rds')
## Make corpus
Corp <- corpus(Acts)
## Add docvars
Corp$period <- attr(Acts, 'period')
Corp$call <- sub('-\\d.*', '', names(Acts)) %>%
sub('^.*-', '', .)  %>%
factor(levels = sort(unique(.)))
Corp$year <- gsub('^.*-\\d{1,2}-|\\:.*$', '', names(Acts)) %>%
substring(1,4) %>% # Only first year
gsub('/', '-', .) %>% factor(levels = sort(unique(.)))
## Tokenize
Toks <- tokens(Acts, remove_punct = T, remove_numbers = TRUE, padding = T) %>%
tokens_remove(pattern = stopwords("en"), padding = T) %>% ## Remove stopwords
tokens_wordstem # Stem words by Martin Porter's stemming algorithm
### Compound multi-word expressions
Colls <- textstat_collocations(Toks, min_count = 10, size = 2, tolower = F) # Collocations
Toks <- tokens_compound(Toks, pattern = Colls[Colls$z > 3]) # 3 ~ p < 0.005
Toks %<>% tokens_remove('') # Remove padding
## Make Document Feature Matrix
dfMat <- dfm(Toks)
#dfMat %<>% dfm_trim(min_termfreq = 10) # Remove features that occur <10 times
## Make Feature Co-occurrence matrix
fcMat <- fcm(dfMat) #%>% fcm_select(pattern = names(topfeatures(fcm, 100)), selection = "keep") # Keep only top 100 features
## Save
save(Corp, Toks, dfMat, fcMat, file = 'corpus.Rda')
?textplot_scale1d
dfMat
docnames(dfMat)
## Correspondence analysis
caMod <- textmodel_ca(dfMat)
docnames(caMod)
str(caMod)
caMod$rowcoord
str(caMod)
caMod$rowcoord$dimnames
attr(caMod$rowcoord)
attr(caMod$rowcoord, 'dimnames')
?attr
?attributes
mostattributes(caMod)
mostattributes(caMod)
?mostattributes
??mostattributes
attr(caMod$rowcoord, 'dimnames')
attr(caMod$rowcoord, 'dimnames')[[1]]
docnames(Corp)
sub(':.*$', '', names(Corp))
## Correspondence analysis
caMod <- textmodel_ca(dfMat)
attr(caMod$rowcoord, 'dimnames')[[1]] <- sub(':.*$', '', names(Corp))
textplot_scale1d(caMod, groups = Corp$call)
attr(caMod$rowcoord, 'dimnames')[[1]]
str(caMod)
caMod$rownames <- sub(':.*$', '', names(Corp))
textplot_scale1d(caMod, groups = Corp$call)
caMod$rownames
Docs <- sapply(list.files(pattern = '.pdf*'), pdf_text)
## Remove some pages
Pages <- list(6:79,
c(17:91, 98:122, 126:155, 159:169),
c(16:86, 93:120, 125:162, 167:199))
for (i in seq_along(Docs)) Docs[[i]] <- Docs[[i]][Pages[[i]]]
## Collapse pages
Docs %<>% sapply(paste, collapse = '')
## Split by activity
splitPat <- '\n\\s*\\p{Uppercase}{2,4}.*-\\d{1,2}-\\d{4,}.*(.*\n){1,5}\\s*Specific' # Define pattern to split by
str_extract_all(Docs[[3]], splitPat) # Examine
Acts <- str_split(Docs, splitPat) %>% lapply(`[`, -1) # Remove part before 1st activity
## Tidy
Acts <- unlist(Acts)
### Add period as an attribute
attributes(Acts) <- str_extract_all(Docs, splitPat) %>% sapply(length) %>%
rep(c("1415", "1617", "1820"), times = .) %>% list(period = .)
### Add document names
names(Acts) <- str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', ' ', .) %>% # Replcae newline characters with space
gsub('\\s+', ' ', .) %>% # Replace spaces with one
sub('\\d*Specific$', '', .) %>% # Remove footnote numbers and "Specific"
gsub('^\\s*|\\s*$', '', .)# Remove leading and trailing spaces
names(Acts)[grep("Ocean literacy", names(Acts))]
#? Created on 2020-09-28 15:16:20 by Jyri Lillemets with R version 4.0.2 (2020-06-22).
#? This script ...
# Set options
#options(device = 'X11')
# Set working directory
setwd('/home/jrl/work/bioeastsup/horizon')
#setwd('C:/Users/lillemets/Google Drive/')
# Load packages
library('magrittr')
library('pdftools');library('stringr')
library('quanteda')
# Download and tidy documents (2020-09-28 15:20:10) ----------
## Load
if (length(list.files(pattern = '.pdf*')) == 0) {
Urls <- c('https://ec.europa.eu/research/participants/data/ref/h2020/wp/2014_2015/main/h2020-wp1415-food_en.pdf',
'https://ec.europa.eu/research/participants/data/ref/h2020/wp/2016_2017/main/h2020-wp1617-food_en.pdf',
'https://ec.europa.eu/research/participants/data/ref/h2020/wp/2018-2020/main/h2020-wp1820-food_en.pdf')
lapply(Urls, function(x) download.file(x, sub('.*\\/','', x)))
}
Docs <- sapply(list.files(pattern = '.pdf*'), pdf_text)
## Remove some pages
Pages <- list(6:79,
c(17:91, 98:122, 126:155, 159:169),
c(16:86, 93:120, 125:162, 167:199))
for (i in seq_along(Docs)) Docs[[i]] <- Docs[[i]][Pages[[i]]]
## Collapse pages
Docs %<>% sapply(paste, collapse = '')
## Split by activity
splitPat <- '\n\\s*\\p{Uppercase}{2,4}.*-\\d{1,2}-\\d{4,}.*(.*\n){1,5}\\s*Specific' # Define pattern to split by
str_extract_all(Docs[[3]], splitPat) # Examine
Acts <- str_split(Docs, splitPat) %>% lapply(`[`, -1) # Remove part before 1st activity
## Tidy
Acts <- unlist(Acts)
### Add period as an attribute
attributes(Acts) <- str_extract_all(Docs, splitPat) %>% sapply(length) %>%
rep(c("1415", "1617", "1820"), times = .) %>% list(period = .)
### Add document names
names(Acts) <- str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', ' ', .) %>% # Replcae newline characters with space
gsub('\\s+', ' ', .) %>% # Replace spaces with one
sub('\\d*Specific$', '', .) %>% # Remove footnote numbers and "Specific"
gsub('^\\s*|\\s*$', '', .)# Remove leading and trailing spaces
### Add colon to a missing case
names(Acts)[names(Acts) == "BG-13-2014 Ocean literacy – Engaging with society – Social Innovation61"] <- "BG-13-2014: Ocean literacy – Engaging with society – Social Innovation61"
## Save
saveRDS(Acts, 'texts.Rds')
# Make corpus, tokens, DFM and FCM (2020-09-28 17:38:58) ----------
## Read data
if (!exists('Acts')) Acts <- readRDS('texts.Rds')
## Make corpus
Corp <- corpus(Acts)
## Add docvars
Corp$period <- attr(Acts, 'period')
Corp$call <- sub('-\\d.*', '', names(Acts)) %>%
sub('^.*-', '', .)  %>%
factor(levels = sort(unique(.)))
Corp$year <- gsub('^.*-\\d{1,2}-|\\:.*$', '', names(Acts)) %>%
substring(1,4) %>% # Only first year
gsub('/', '-', .) %>% factor(levels = sort(unique(.)))
## Tokenize
Toks <- tokens(Acts, remove_punct = T, remove_numbers = TRUE, padding = T) %>%
tokens_remove(pattern = stopwords("en"), padding = T) %>% ## Remove stopwords
tokens_wordstem # Stem words by Martin Porter's stemming algorithm
### Compound multi-word expressions
Colls <- textstat_collocations(Toks, min_count = 10, size = 2, tolower = F) # Collocations
Toks <- tokens_compound(Toks, pattern = Colls[Colls$z > 3]) # 3 ~ p < 0.005
Toks %<>% tokens_remove('') # Remove padding
## Make Document Feature Matrix
dfMat <- dfm(Toks)
#dfMat %<>% dfm_trim(min_termfreq = 10) # Remove features that occur <10 times
## Make Feature Co-occurrence matrix
fcMat <- fcm(dfMat) #%>% fcm_select(pattern = names(topfeatures(fcm, 100)), selection = "keep") # Keep only top 100 features
## Save
save(Corp, Toks, dfMat, fcMat, file = 'corpus.Rda')
## Correspondence analysis
caMod <- textmodel_ca(dfMat)
caMod$rownames <- sub(':.*$', '', names(Corp))
textplot_scale1d(caMod, groups = Corp$call)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', ' ', .) %>% # Replcae newline characters with space
gsub('\\s+', ' ', .) %>% # Replace spaces with one
sub('\\d*Specific$', '', .) %>% # Remove footnote numbers and "Specific"
gsub('^\\s*|\\s*$', '', .)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', ' ', .) %>% # Replcae newline characters with space
gsub('\\s+', ' ', .) %>% # Replace spaces with one
sub('\\d*\n\\s*Specific$', '', .) %>% # Remove footnote numbers and "Specific"
gsub('^\\s*|\\s*$', '', .)
str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', ' ', .) %>% # Replcae newline characters with space
gsub('\\s+', ' ', .) %>% # Replace spaces with one
sub('\\d*\n*\\s*Specific$', '', .) %>% # Remove footnote numbers and "Specific"
gsub('^\\s*|\\s*$', '', .)
#? Created on 2020-09-28 15:16:20 by Jyri Lillemets with R version 4.0.2 (2020-06-22).
#? This script ...
# Set options
#options(device = 'X11')
# Set working directory
setwd('/home/jrl/work/bioeastsup/horizon')
#setwd('C:/Users/lillemets/Google Drive/')
# Load packages
library('magrittr')
library('pdftools');library('stringr')
library('quanteda')
# Download and tidy documents (2020-09-28 15:20:10) ----------
## Load
if (length(list.files(pattern = '.pdf*')) == 0) {
Urls <- c('https://ec.europa.eu/research/participants/data/ref/h2020/wp/2014_2015/main/h2020-wp1415-food_en.pdf',
'https://ec.europa.eu/research/participants/data/ref/h2020/wp/2016_2017/main/h2020-wp1617-food_en.pdf',
'https://ec.europa.eu/research/participants/data/ref/h2020/wp/2018-2020/main/h2020-wp1820-food_en.pdf')
lapply(Urls, function(x) download.file(x, sub('.*\\/','', x)))
}
Docs <- sapply(list.files(pattern = '.pdf*'), pdf_text)
## Remove some pages
Pages <- list(6:79,
c(17:91, 98:122, 126:155, 159:169),
c(16:86, 93:120, 125:162, 167:199))
for (i in seq_along(Docs)) Docs[[i]] <- Docs[[i]][Pages[[i]]]
## Collapse pages
Docs %<>% sapply(paste, collapse = '')
## Split by activity
splitPat <- '\n\\s*\\p{Uppercase}{2,4}.*-\\d{1,2}-\\d{4,}.*(.*\n){1,5}\\s*Specific' # Define pattern to split by
str_extract_all(Docs[[3]], splitPat) # Examine
Acts <- str_split(Docs, splitPat) %>% lapply(`[`, -1) # Remove part before 1st activity
## Tidy
Acts <- unlist(Acts)
### Add period as an attribute
attributes(Acts) <- str_extract_all(Docs, splitPat) %>% sapply(length) %>%
rep(c("1415", "1617", "1820"), times = .) %>% list(period = .)
### Add document names
names(Acts) <- str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', ' ', .) %>% # Replcae newline characters with space
gsub('\\s+', ' ', .) %>% # Replace spaces with one
sub('\\d*\n*\\s*Specific$', '', .) %>% # Remove footnote numbers and "Specific"
gsub('^\\s*|\\s*$', '', .) # Remove leading and trailing spaces
### Add colon to a missing case
names(Acts)[names(Acts) == "BG-13-2014 Ocean literacy – Engaging with society – Social Innovation"] <- "BG-13-2014: Ocean literacy – Engaging with society – Social Innovation"
## Save
saveRDS(Acts, 'texts.Rds')
# Make corpus, tokens, DFM and FCM (2020-09-28 17:38:58) ----------
## Read data
if (!exists('Acts')) Acts <- readRDS('texts.Rds')
## Make corpus
Corp <- corpus(Acts)
## Add docvars
Corp$period <- attr(Acts, 'period')
Corp$call <- sub('-\\d.*', '', names(Acts)) %>%
sub('^.*-', '', .)  %>%
factor(levels = sort(unique(.)))
Corp$year <- gsub('^.*-\\d{1,2}-|\\:.*$', '', names(Acts)) %>%
substring(1,4) %>% # Only first year
gsub('/', '-', .) %>% factor(levels = sort(unique(.)))
## Tokenize
Toks <- tokens(Acts, remove_punct = T, remove_numbers = TRUE, padding = T) %>%
tokens_remove(pattern = stopwords("en"), padding = T) %>% ## Remove stopwords
tokens_wordstem # Stem words by Martin Porter's stemming algorithm
### Compound multi-word expressions
Colls <- textstat_collocations(Toks, min_count = 10, size = 2, tolower = F) # Collocations
Toks <- tokens_compound(Toks, pattern = Colls[Colls$z > 3]) # 3 ~ p < 0.005
Toks %<>% tokens_remove('') # Remove padding
## Make Document Feature Matrix
dfMat <- dfm(Toks)
#dfMat %<>% dfm_trim(min_termfreq = 10) # Remove features that occur <10 times
## Make Feature Co-occurrence matrix
fcMat <- fcm(dfMat) #%>% fcm_select(pattern = names(topfeatures(fcm, 100)), selection = "keep") # Keep only top 100 features
## Save
save(Corp, Toks, dfMat, fcMat, file = 'corpus.Rda')
str(Toks)
#? Created on 2020-09-28 15:16:20 by Jyri Lillemets with R version 4.0.2 (2020-06-22).
#? This script ...
# Set options
#options(device = 'X11')
# Set working directory
setwd('/home/jrl/work/bioeastsup/horizon')
#setwd('C:/Users/lillemets/Google Drive/')
# Load packages
library('magrittr')
library('pdftools');library('stringr')
library('quanteda')
# Download and tidy documents (2020-09-28 15:20:10) ----------
## Load
if (length(list.files(pattern = '.pdf*')) == 0) {
Urls <- c('https://ec.europa.eu/research/participants/data/ref/h2020/wp/2014_2015/main/h2020-wp1415-food_en.pdf',
'https://ec.europa.eu/research/participants/data/ref/h2020/wp/2016_2017/main/h2020-wp1617-food_en.pdf',
'https://ec.europa.eu/research/participants/data/ref/h2020/wp/2018-2020/main/h2020-wp1820-food_en.pdf')
lapply(Urls, function(x) download.file(x, sub('.*\\/','', x)))
}
Docs <- sapply(list.files(pattern = '.pdf*'), pdf_text)
## Remove some pages
Pages <- list(6:79,
c(17:91, 98:122, 126:155, 159:169),
c(16:86, 93:120, 125:162, 167:199))
for (i in seq_along(Docs)) Docs[[i]] <- Docs[[i]][Pages[[i]]]
## Collapse pages
Docs %<>% sapply(paste, collapse = '')
## Split by activity
splitPat <- '\n\\s*\\p{Uppercase}{2,4}.*-\\d{1,2}-\\d{4,}.*(.*\n){1,5}\\s*Specific' # Define pattern to split by
str_extract_all(Docs[[3]], splitPat) # Examine
Acts <- str_split(Docs, splitPat) %>% lapply(`[`, -1) # Remove part before 1st activity
## Tidy
Acts <- unlist(Acts)
### Add period as an attribute
attributes(Acts) <- str_extract_all(Docs, splitPat) %>% sapply(length) %>%
rep(c("1415", "1617", "1820"), times = .) %>% list(period = .)
### Add document names
names(Acts) <- str_extract_all(Docs, splitPat) %>% unlist %>%
gsub('\n', ' ', .) %>% # Replcae newline characters with space
gsub('\\s+', ' ', .) %>% # Replace spaces with one
sub('\\d*\n*\\s*Specific$', '', .) %>% # Remove footnote numbers and "Specific"
gsub('^\\s*|\\s*$', '', .) # Remove leading and trailing spaces
### Add colon to a missing case
names(Acts)[names(Acts) == "BG-13-2014 Ocean literacy – Engaging with society – Social Innovation"] <- "BG-13-2014: Ocean literacy – Engaging with society – Social Innovation"
## Save
saveRDS(Acts, 'texts.Rds')
# Make corpus, tokens, DFM and FCM (2020-09-28 17:38:58) ----------
## Read data
if (!exists('Acts')) Acts <- readRDS('texts.Rds')
## Make corpus
Corp <- corpus(Acts)
## Add docvars
Corp$period <- attr(Acts, 'period')
Corp$call <- sub('-\\d.*', '', names(Acts)) %>%
sub('^.*-', '', .)  %>%
factor(levels = sort(unique(.)))
Corp$year <- gsub('^.*-\\d{1,2}-|\\:.*$', '', names(Acts)) %>%
substring(1,4) %>% # Only first year
gsub('/', '-', .) %>% factor(levels = sort(unique(.)))
## Tokenize
Toks <- tokens(Acts, remove_punct = T, remove_numbers = TRUE, padding = T) %>%
tokens_remove(pattern = stopwords("en"), padding = T) %>% ## Remove stopwords
tokens_wordstem # Stem words by Martin Porter's stemming algorithm
### Compound multi-word expressions
Colls <- textstat_collocations(Toks, min_count = 10, size = 2, tolower = F) # Collocations
Toks <- tokens_compound(Toks, pattern = Colls[Colls$z > 3]) # 3 ~ p < 0.005
Toks %<>% tokens_remove('') # Remove padding
## Make Document Feature Matrix
dfMat <- dfm(Toks)
#dfMat %<>% dfm_trim(min_termfreq = 10) # Remove features that occur <10 times
## Make Feature Co-occurrence matrix
fcMat <- fcm(dfMat) #%>% fcm_select(pattern = names(topfeatures(fcm, 100)), selection = "keep") # Keep only top 100 features
## Save
save(Corp, Toks, dfMat, fcMat, file = 'corpus.Rda')
